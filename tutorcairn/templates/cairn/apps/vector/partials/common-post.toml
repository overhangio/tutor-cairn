### Transforms

# Parse tracking logs: extract time
[transforms.tracking]
type = "remap"
inputs = ["openedx_containers"]
# Time formats: https://docs.rs/chrono/0.4.19/chrono/format/strftime/index.html#specifiers
source = '''
parsed, err_regex = parse_regex(.message, r'^.* \[tracking\] [^{}]* (?P<tracking_message>\{.*\})$')
if err_regex != null {
  abort
}
message = parsed.tracking_message
parsed_json, err_json = parse_json(parsed.tracking_message)
if err_json != null {
  log("Unable to parse JSON from tracking log message: " + err_json, level: "error")
  abort
}
time, err_timestamp = parse_timestamp(parsed_json.time, "%+")
if err_timestamp != null {
  log("Unable to parse timestamp from tracking log 'time' field: " + err_timestamp, level: "warn")
  time, err_timestamp = parse_timestamp(parsed_json.timestamp, "%+")
  if err_timestamp != null {
    log("Unable to parse timestamp from tracking log 'timestamp' field: " + err_timestamp, level: "error")
    abort
  }
}
. = {"time": time, "message": message}
'''
drop_on_error = true
drop_on_abort = true

[transforms.tracking_debug]
type = "remap"
inputs = ["tracking"]
# Time formats: https://docs.rs/chrono/0.4.19/chrono/format/strftime/index.html#specifiers
source = '''
.message = parse_json!(.message)
'''

# Parse CMS logs for course publishing event
[transforms.course_published]
type="remap"
inputs = ["openedx_containers"]
source = '''
parsed, err_regex = parse_regex(.message, r'Updating course overview for (?P<course_id>\S+?)(?:\s|\.|$)')
  if err_regex != null {
    log("Unable to parse course_id from log message: " +  err_regex, level: "error")
    abort
  }
. = {"course_id": parsed.course_id}
'''
drop_on_error = true
drop_on_abort = true

### Sinks

# Log all events to stdout, for debugging
[sinks.out]
type = "console"
inputs = ["tracking_debug"]
encoding.codec = "json"
encoding.only_fields = ["time", "message.context.course_id", "message.context.user_id", "message.name"]

# # Send logs to clickhouse
[sinks.clickhouse]
type = "clickhouse"
# Required: https://github.com/timberio/vector/issues/5797
encoding.timestamp_format = "unix"
inputs = ["tracking"]
endpoint = "{{ CAIRN_CLICKHOUSE_HTTP_SCHEME }}://{{ CAIRN_CLICKHOUSE_HOST }}:{{ CAIRN_CLICKHOUSE_HTTP_PORT }}"
database = "{{ CAIRN_CLICKHOUSE_DATABASE }}"
table = "_tracking"
healthcheck = true

# Log course_published event to stdout for debugging
[sinks.course_published_out]
type = "console"
inputs = ["course_published"]
encoding.codec = "json"
encoding.only_fields = ["course_id"]

# Send course_id to watchcourses
[sinks.watchcourse]
type = "http"
method = "post"
encoding.codec = "json"
inputs = ["course_published"]
# Batch events together to reduce the number of times
# the importcoursedata script is run
# Vector will wait 300 secs (5 mins) from the first event
# or until there are 10 events to trigger the watchcourses service
batch.timeout_secs = 300
batch.max_events = 10
uri = "http://cairn-watchcourses:9282/courses/published/"

{{ patch("cairn-vector-common-toml") }}
